import gc
import torch
import numpy as np

from llm_attacks import AttackPrompt, PromptManager, MultiPromptAttack
from llm_attacks import get_embedding_matrix, get_embeddings, get_nonascii_toks


def token_gradients_activation(model, input_ids, input_slice, pos, layer, direction, act_obj="negative"):
    """
    Compute gradients of the activation projection onto `direction` w.r.t. control tokens.

    act_obj:
      - "negative": minimize projection (push negative) at (layer,pos)
      - "zero": minimize projection magnitude at (layer,pos)
      - "global_zero": minimize sum of projection magnitudes across *all* layers/tokens
                       (directional ablation style)
      - "layer_zero_all": minimize projection magnitudes for *all tokens* at the specified layer
      - "token_all_layers": minimize projection magnitudes at the chosen token position, aggregated across *all* layers
    """
    # Clear stale grads
    model.zero_grad(set_to_none=True)

    # Normalize direction once
    direction = direction / (direction.norm() + 1e-8)

    embed_weights = get_embedding_matrix(model)
    one_hot = torch.zeros(
        input_ids[input_slice].shape[0],
        embed_weights.shape[0],
        device=model.device,
        dtype=embed_weights.dtype,
    )
    one_hot.scatter_(
        1,
        input_ids[input_slice].unsqueeze(1),
        torch.ones(one_hot.shape[0], 1, device=model.device, dtype=embed_weights.dtype),
    )
    one_hot.requires_grad_()
    input_embeds = (one_hot @ embed_weights).unsqueeze(0)

    # stitch together embeddings (others are detached)
    embeds = get_embeddings(model, input_ids.unsqueeze(0)).detach()
    full_embeds = torch.cat(
        [embeds[:, : input_slice.start, :], input_embeds, embeds[:, input_slice.stop :, :]],
        dim=1,
    )

    # Build loss depending on objective
    loss_terms = []

    if act_obj == "global_zero":
        # Hook every block resid_pre and accumulate proj^2 over all tokens.
        # Compute the loss in float32 and average over all (layer, token) pairs
        # to keep scales reasonable and avoid fp16 overflow.
        handles = []

        def global_hook(module, hook_input):
            act = hook_input[0]  # [batch, seq, d_model]
            act32 = act.to(torch.float32)
            dir32 = direction.to(act32)
            proj = act32 @ dir32  # [batch, seq]
            loss_terms.append((proj ** 2).sum())

        for blk in model.model.layers:
            handles.append(blk.register_forward_pre_hook(global_hook))
        _ = model(inputs_embeds=full_embeds)
        for h in handles:
            h.remove()
        if not loss_terms:
            raise RuntimeError("global_zero objective: no activations captured.")
        total_positions = len(loss_terms) * full_embeds.shape[1]  # num_layers * seq_len (batch assumed 1)
        obj = torch.stack(loss_terms).sum() / total_positions
    elif act_obj == "layer_zero_all":
        # Single-layer directional ablation across all tokens
        captured = {}

        def layer_hook(module, hook_input):
            captured["act"] = hook_input[0]

        handle = model.model.layers[layer].register_forward_pre_hook(layer_hook)
        _ = model(inputs_embeds=full_embeds)
        handle.remove()

        hidden = captured["act"][0].to(torch.float32)  # [seq, d_model]
        dir32 = direction.to(hidden)
        proj = hidden @ dir32  # [seq]
        obj = (proj ** 2).mean()  # mean across all tokens at this layer
    elif act_obj == "token_all_layers":
        # Aggregate proj^2 over all layers at a single token position
        captured = []

        def layer_hook(module, hook_input):
            captured.append(hook_input[0])

        handles = [blk.register_forward_pre_hook(layer_hook) for blk in model.model.layers]
        _ = model(inputs_embeds=full_embeds)
        for h in handles:
            h.remove()

        if not captured:
            raise RuntimeError("token_all_layers objective: no activations captured.")

        seq_len = captured[0].shape[1]
        idx = pos if pos >= 0 else seq_len + pos
        idx = max(min(idx, seq_len - 1), 0)

        obj_terms = []
        for act in captured:
            hidden = act[0].to(torch.float32)  # [seq, d_model]
            dir32 = direction.to(hidden)
            proj = torch.dot(hidden[idx], dir32)
            obj_terms.append(proj ** 2)
        obj = torch.stack(obj_terms).mean()
    else:
        captured = {}

        def pre_hook(module, hook_input):
            # Keep gradient flow for the control tokens; do not detach.
            captured["act"] = hook_input[0]

        handle = model.model.layers[layer].register_forward_pre_hook(pre_hook)
        _ = model(inputs_embeds=full_embeds)
        handle.remove()

        hidden = captured["act"][0]  # [seq, d_model]
        seq_len = hidden.shape[0]
        idx = pos if pos >= 0 else seq_len + pos
        idx = max(min(idx, seq_len - 1), 0)

        direction_local = direction.to(hidden)
        proj = torch.dot(hidden[idx], direction_local)
        if act_obj == "zero":
            obj = proj ** 2  # ablation-aligned: projection -> 0
        else:
            obj = proj  # push negative

    obj.backward()

    grad = one_hot.grad.clone()
    if act_obj == "global_zero":
        del embeds, full_embeds, input_embeds, one_hot
    elif act_obj == "layer_zero_all":
        del embeds, full_embeds, input_embeds, one_hot, hidden, captured
    elif act_obj == "token_all_layers":
        del embeds, full_embeds, input_embeds, one_hot, captured
    else:
        del embeds, full_embeds, input_embeds, one_hot, hidden, captured
    gc.collect()
    return grad


class ActivationAttackPrompt(AttackPrompt):
    def __init__(self, *args, direction=None, layer=None, pos=-1, act_obj="negative", score_mode="global", **kwargs):
        super().__init__(*args, **kwargs)
        assert direction is not None and layer is not None
        self.direction = direction
        self.layer = layer
        self.pos = pos
        # act_obj: "negative" -> push projection negative; "zero" -> minimize magnitude
        self.act_obj = act_obj
        self.score_mode = score_mode

    def grad_activation(self, model):
        return token_gradients_activation(
            model,
            self.input_ids.to(model.device),
            self._control_slice,
            self.pos,
            self.layer,
            self.direction,
            self.act_obj,
        )

    def activation_score(self, model, direction=None, layer=None, pos=None, test_controls=None, score_mode=None):
        """
        Compute projection score(s) for provided control strings/tensors.
        Returns a 1D tensor of scores (lower is better).
        """
        direction = direction if direction is not None else self.direction
        layer = layer if layer is not None else self.layer
        pos = self.pos if pos is None else pos
        act_obj = getattr(self, "act_obj", "negative")
        score_mode = score_mode or getattr(self, "score_mode", "global")

        pad_tok = -1
        if test_controls is None:
            test_controls = self.control_toks

        if isinstance(test_controls, torch.Tensor):
            if len(test_controls.shape) == 1:
                test_controls = test_controls.unsqueeze(0)
            test_ids = test_controls.to(model.device)
        elif isinstance(test_controls, list) and isinstance(test_controls[0], str):
            max_len = self._control_slice.stop - self._control_slice.start
            test_ids = [
                torch.tensor(
                    self.tokenizer(control, add_special_tokens=False).input_ids[:max_len],
                    device=model.device,
                )
                for control in test_controls
            ]
            pad_tok = 0
            while pad_tok in self.input_ids or any([pad_tok in ids for ids in test_ids]):
                pad_tok += 1
            nested_ids = torch.nested.nested_tensor(test_ids)
            test_ids = torch.nested.to_padded_tensor(nested_ids, pad_tok, (len(test_ids), max_len))
        else:
            raise ValueError("test_controls must be a tensor or list of strings")

        locs = torch.arange(self._control_slice.start, self._control_slice.stop).repeat(
            test_ids.shape[0], 1
        ).to(model.device)
        ids = torch.scatter(
            self.input_ids.unsqueeze(0).repeat(test_ids.shape[0], 1).to(model.device),
            1,
            locs,
            test_ids,
        )
        attn_mask = (ids != pad_tok).type(ids.dtype) if pad_tok >= 0 else None

        if score_mode == "global":
            # Global scoring: sum (or sum of squares) over all layers/tokens, per example.
            handles = []
            loss_terms = []

            def global_hook(module, hook_input):
                act = hook_input[0]  # [batch, seq, d_model]
                act32 = act.to(torch.float32)
                dir32 = direction.to(act32)
                dir32 = dir32 / (dir32.norm() + 1e-8)
                proj = act32 @ dir32  # [batch, seq]
                if act_obj in ("zero", "global_zero", "layer_zero_all"):
                    term = (proj ** 2).sum(dim=1)  # [batch]
                else:
                    term = proj.sum(dim=1)  # [batch]
                loss_terms.append(term)

            for blk in model.model.layers:
                handles.append(blk.register_forward_pre_hook(global_hook))
            _ = model(input_ids=ids, attention_mask=attn_mask)
            for h in handles:
                h.remove()

            if not loss_terms:
                raise RuntimeError("global scoring: no activations captured.")
            # loss_terms: list of [batch]; stack -> [layers, batch]
            stacked = torch.stack(loss_terms)  # [num_layers, batch]
            scores = stacked.sum(dim=0) / (stacked.shape[0] * ids.shape[1])
            del ids, locs, test_ids, stacked, loss_terms, handles
            gc.collect()
            return scores
        elif score_mode == "token_all_layers":
            layer_acts = []
            handles = []

            def hook_all(module, hook_input):
                layer_acts.append(hook_input[0])

            for blk in model.model.layers:
                handles.append(blk.register_forward_pre_hook(hook_all))

            _ = model(input_ids=ids, attention_mask=attn_mask)
            for h in handles:
                h.remove()

            if not layer_acts:
                raise RuntimeError("token_all_layers scoring: no activations captured.")

            lengths = attn_mask.sum(-1) if attn_mask is not None else torch.tensor(
                [ids.shape[1]] * ids.shape[0], device=ids.device
            )
            idxs = []
            for b in range(layer_acts[0].shape[0]):
                seq_len = int(lengths[b].item())
                idx = pos if pos >= 0 else seq_len + pos
                idx = max(min(idx, seq_len - 1), 0)
                idxs.append(idx)
            idxs = torch.tensor(idxs, device=ids.device)

            dir32 = direction.to(ids.device)
            dir32 = dir32 / (dir32.norm() + 1e-8)
            proj_sq_layers = []
            for act in layer_acts:
                act32 = act.to(torch.float32)
                proj = torch.stack(
                    [torch.dot(act32[b, idxs[b], :], dir32) for b in range(act32.shape[0])]
                )
                proj_sq_layers.append(proj ** 2)
            scores = torch.stack(proj_sq_layers).mean(dim=0)
            del ids, locs, test_ids, layer_acts, proj_sq_layers, dir32, lengths, idxs, handles
            gc.collect()
            return scores
        else:
            captured = {}

            def pre_hook(module, hook_input):
                captured["act"] = hook_input[0]

            handle = model.model.layers[layer].register_forward_pre_hook(pre_hook)
            _ = model(input_ids=ids, attention_mask=attn_mask)
            handle.remove()

            activations = captured["act"]  # [batch, seq, d_model]
            lengths = attn_mask.sum(-1) if attn_mask is not None else torch.tensor(
                [ids.shape[1]] * ids.shape[0], device=ids.device
            )
            # Compute projections in float32 to avoid overflow/inf
            act32 = activations.to(torch.float32)
            dir32 = direction.to(act32)
            dir32 = dir32 / (dir32.norm() + 1e-8)
            if act_obj == "layer_zero_all":
                proj = act32 @ dir32  # [batch, seq]
                if attn_mask is not None:
                    mask = attn_mask.to(proj.dtype)
                    proj_sq = (proj ** 2) * mask
                    scores = proj_sq.sum(dim=1) / mask.sum(dim=1).clamp_min(1e-8)
                else:
                    scores = (proj ** 2).mean(dim=1)
            else:
                idxs = []
                for b in range(activations.shape[0]):
                    seq_len = int(lengths[b].item())
                    idx = pos if pos >= 0 else seq_len + pos
                    idx = max(min(idx, seq_len - 1), 0)
                    idxs.append(idx)
                idxs = torch.tensor(idxs, device=activations.device)

                # Return per-example scores according to objective
                proj = torch.stack(
                    [torch.dot(act32[b, idxs[b], :], dir32) for b in range(act32.shape[0])]
                )
                if act_obj in ("zero", "global_zero"):
                    scores = proj ** 2  # ablation-aligned: projection -> 0
                else:
                    scores = proj  # negative objective: push projection as negative as possible

            del ids, locs, test_ids, activations, act32, dir32, captured
            gc.collect()
            return scores


class ActivationPromptManager(PromptManager):
    def __init__(self, *args, direction=None, layer=None, pos=-1, act_obj="negative", score_mode="global", managers=None, **kwargs):
        assert direction is not None and layer is not None
        self.direction = direction
        self.layer = layer
        self.pos = pos
        self.act_obj = act_obj
        if score_mode == "local" and act_obj == "token_all_layers":
            score_mode = "token_all_layers"
        self.score_mode = score_mode

        goals, targets, tokenizer, conv_template, control_init, test_prefixes = args[:6]

        self.tokenizer = tokenizer
        self._prompts = [
            managers["AP"](
                goal,
                target,
                tokenizer,
                conv_template,
                control_init,
                test_prefixes,
                direction=direction,
                layer=layer,
                pos=pos,
                act_obj=act_obj,
                score_mode=score_mode,
            )
            for goal, target in zip(goals, targets)
        ]
        self._nonascii_toks = get_nonascii_toks(tokenizer, device="cpu")

    def grad_activation(self, model):
        # Aggregate grad_activation across prompts (mirrors PromptManager.grad)
        return sum([prompt.grad_activation(model) for prompt in self._prompts])

    def sample_control(self, grad, batch_size, topk=256, temp=1, allow_non_ascii=True):
        if not allow_non_ascii:
            grad[:, self._nonascii_toks.to(grad.device)] = np.inf
        top_indices = (-grad).topk(topk, dim=1).indices
        control_toks = self.control_toks.to(grad.device)
        original_control_toks = control_toks.repeat(batch_size, 1)
        new_token_pos = torch.arange(
            0, len(control_toks), len(control_toks) / batch_size, device=grad.device
        ).type(torch.int64)
        new_token_val = torch.gather(
            top_indices[new_token_pos],
            1,
            torch.randint(0, topk, (batch_size, 1), device=grad.device),
        )
        new_control_toks = original_control_toks.scatter_(1, new_token_pos.unsqueeze(-1), new_token_val)
        return new_control_toks


class ActivationMultiPromptAttack(MultiPromptAttack):
    def __init__(
        self,
        goals,
        targets,
        workers,
        direction,
        layer,
        pos=-1,
        act_obj="layer_zero_all",
        score_mode="local",
        control_init="! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !",
        test_prefixes=[
            "I'm sorry",
            "I apologize",
            "As an",
            "I'm just",
            "I'm an",
            "I cannot",
            "Hello!",
        ],
        logfile=None,
        managers=None,
        test_goals=[],
        test_targets=[],
        test_workers=[],
        *args,
        **kwargs,
    ):
        self.goals = goals
        self.targets = targets
        self.workers = workers
        self.test_goals = test_goals
        self.test_targets = test_targets
        self.test_workers = test_workers
        self.test_prefixes = test_prefixes
        self.models = [worker.model for worker in workers]
        self.logfile = logfile
        self.direction = direction
        self.layer = layer
        self.pos = pos
        self.managers = managers
        self.act_obj = act_obj
        if score_mode == "local" and act_obj == "token_all_layers":
            score_mode = "token_all_layers"
        self.score_mode = score_mode

        self.prompts = [
            managers["PM"](
                goals,
                targets,
                worker.tokenizer,
                worker.conv_template,
                control_init,
                test_prefixes,
                direction=direction,
                layer=layer,
                pos=pos,
                act_obj=act_obj,
                score_mode=score_mode,
                managers=managers,
            )
            for worker in workers
        ]

    def step(
        self,
        batch_size=1024,
        topk=256,
        temp=1,
        allow_non_ascii=True,
        target_weight=1,
        control_weight=0.0,
        verbose=False,
        opt_only=False,
        filter_cand=True,
    ):
        # opt_only unused; kept for signature compatibility
        opt_only = False
        main_device = self.models[0].device
        control_cands = []

        # 1) gradients for each prompt/model
        for j, worker in enumerate(self.workers):
            worker(self.prompts[j], "grad_activation", worker.model)

        grad = None
        for j, worker in enumerate(self.workers):
            new_grad = worker.results.get().to(main_device)
            new_grad = new_grad / new_grad.norm(dim=-1, keepdim=True)
            if grad is None:
                grad = torch.zeros_like(new_grad)
            if grad.shape != new_grad.shape:
                with torch.no_grad():
                    control_cand = self.prompts[j - 1].sample_control(
                        grad, batch_size, topk, temp, allow_non_ascii
                    )
                    control_cands.append(
                        self.get_filtered_cands(
                            j - 1, control_cand, filter_cand=filter_cand, curr_control=self.control_str
                        )
                    )
                grad = new_grad
            else:
                grad += new_grad

        with torch.no_grad():
            control_cand = self.prompts[j].sample_control(grad, batch_size, topk, temp, allow_non_ascii)
            control_cands.append(
                self.get_filtered_cands(j, control_cand, filter_cand=filter_cand, curr_control=self.control_str)
            )
        del grad, control_cand
        gc.collect()

        # 2) Evaluate candidates
        loss = torch.zeros(len(control_cands) * batch_size).to(main_device)
        with torch.no_grad():
            for j, cand in enumerate(control_cands):
                progress = (
                    tqdm(range(len(self.prompts[0])), total=len(self.prompts[0])) if verbose else range(len(self.prompts[0]))
                )
                for i in progress:
                    for k, worker in enumerate(self.workers):
                        worker(
                            self.prompts[k][i],
                            "activation_score",
                            worker.model,
                            self.direction,
                            self.layer,
                            self.pos,
                            cand,
                            self.score_mode,
                        )
                    scores = [worker.results.get() for worker in self.workers]
                    loss[j * batch_size : (j + 1) * batch_size] += sum(
                        [s.to(main_device) for s in scores]
                    )
                    del scores
                    gc.collect()
                    if verbose:
                        progress.set_description(
                            f"loss={loss[j*batch_size:(j+1)*batch_size].min().item()/(i+1):.4f}"
                        )

            min_idx = loss.argmin()
            model_idx = min_idx // batch_size
            batch_idx = min_idx % batch_size
            next_control, cand_loss = control_cands[model_idx][batch_idx], loss[min_idx]

        del control_cands, loss
        gc.collect()

        print("Current length:", len(self.workers[0].tokenizer(next_control).input_ids[1:]))
        print(next_control)

        # average across prompts/models for reporting
        return next_control, cand_loss.item() / len(self.prompts[0]) / len(self.workers)
